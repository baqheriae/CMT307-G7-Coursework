# -*- coding: utf-8 -*-
"""pre_trained_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Lzrj6DBDiLPdl8if89MJwNH2f_80s8x

Load data
"""

# load data
import pickle
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt


with open('data_2.pickle', 'rb') as f:
    X = pickle.load(f)
with open('data_y_2.pickle', 'rb') as f:
    y = pickle.load(f)
#Split data into training, test and validation(75% 15% 10%)
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)
X_valid,X_test,y_valid,y_test = train_test_split(X_test,y_test,test_size=0.4)
#Randomly shuffle data
np.random.seed(200)
np.random.shuffle(X_train)
np.random.seed(200)
np.random.shuffle(X_test)
np.random.seed(200)
np.random.shuffle(y_train)
np.random.seed(200)
np.random.shuffle(y_test)
np.random.seed(200)
np.random.shuffle(X_valid)
np.random.seed(200)
np.random.shuffle(y_valid)

"""Normalized"""

x_train = X_train / 255.0
x_test = X_test / 255.0
x_valid = X_valid / 255.0

#Fine-tuning xception model

#Training with existing layers fixed
#activation function = softmax
#the Arguments means of Xception can be found at https://keras.io/applications/
base_model = keras.applications.xception.Xception(weights="imagenet", include_top=False)
avg = keras.layers.GlobalAveragePooling2D()(base_model.output)
output = keras.layers.Dense(8, activation="softmax")(avg)
model = keras.Model(inputs=base_model.input, outputs=output)
# the Arguments means of optimizers can be found at https://keras.io/optimizers/
for layer in base_model.layers:
  layer.trainable = False
optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
history = model.fit(x_train.reshape((x_train.shape[0], 200, 200, 3)), y_train, epochs=5, validation_data=(x_valid.reshape((x_valid.shape[0], 200, 200, 3)),y_valid))

#The model can be further trained with base layers unfrozen
for layer in base_model.layers:
  layer.trainable = True
#the Arguments means of compile can be found at https://keras.io/models/model/
optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
history = model.fit(x_train.reshape((x_train.shape[0], 200, 200, 3)), y_train, epochs=40, validation_data=(x_valid.reshape((x_valid.shape[0], 200, 200, 3)),y_valid))

#Test

model.evaluate(x_test.reshape(x_test.shape[0], 200, 200, 3), y_test)

#save

model.save("model-xception-1.hdf5")

#Draw chart



plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])

plt.title('Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Valid'], loc = 'upper left')

plt.show()


plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Valid'], loc = 'upper left')

plt.show()